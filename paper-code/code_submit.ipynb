{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7709f45-5802-4200-8eba-4f0d06d6a9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi\n",
    "# !pip install diffusers==0.3.0\n",
    "# !pip install transformers scipy ftfy\n",
    "# !pip install \"ipywidgets>=7,<8\"\n",
    "\n",
    "import pdb\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from tqdm.auto import tqdm\n",
    "from torch import autocast\n",
    "from PIL import Image\n",
    "import pdb\n",
    "\n",
    "YOUR_TOKEN=\"REPLACE WITH YOUR HUGGINGFACE TOKEN\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", use_auth_token=YOUR_TOKEN)  \n",
    "pipe = pipe.to(\"cuda\")\n",
    "\n",
    "tokenizer, text_encoder, unet, scheduler, vae = pipe.tokenizer, pipe.text_encoder, pipe.unet, pipe.scheduler, pipe.vae\n",
    "\n",
    "max_length = tokenizer.model_max_length\n",
    "torch_device = 'cuda'\n",
    "batch_size = 1\n",
    "\n",
    "\n",
    "def get_text_embeddings(prompt):\n",
    "    text_input = tokenizer(prompt, padding=\"max_length\", max_length=max_length, truncation=True, return_tensors=\"pt\")\n",
    "    text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
    "    \n",
    "    return text_embeddings\n",
    "\n",
    "def im_gen(text_embeddings, latents, num_inference_steps=200, guidance_scale=10): \n",
    "    uncond_input = tokenizer(\n",
    "      [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
    "    )\n",
    "    uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n",
    "    text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "    # latents = latents * scheduler.init_noise_sigma\n",
    "    latents = latents.to(torch_device) \n",
    "\n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "    with autocast(\"cuda\"):\n",
    "        for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
    "            # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "            latent_model_input = torch.cat([latents] * 2)\n",
    "            # predict the noise residual\n",
    "            with torch.no_grad():\n",
    "                noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "                # perform guidance\n",
    "                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "                # compute the previous noisy sample x_t -> x_t-1\n",
    "                latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "\n",
    "    with torch.no_grad():\n",
    "        latents = 1 / 0.18215 * latents\n",
    "        image = vae.decode(latents).sample\n",
    "\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)\n",
    "    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "    images = (image * 255).round().astype(\"uint8\")\n",
    "\n",
    "    return Image.fromarray(images[0])\n",
    "\n",
    "def transfer_score(emb0, emb_add, emb_minus, latents, strength = 1, \n",
    "                   num_inference_steps=200, guidance_scale=10, correction_scale = 1): \n",
    "    uncond_input = tokenizer(\n",
    "      [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
    "    )\n",
    "    uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]    \n",
    "\n",
    "    # latents = latents * scheduler.init_noise_sigma\n",
    "    latents = latents.to(torch_device) \n",
    "\n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "    t0 = int(num_inference_steps * strength)\n",
    "    first_stage_steps = num_inference_steps - t0\n",
    "    \n",
    "    with autocast(\"cuda\"):\n",
    "        for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
    "            with torch.no_grad():\n",
    "                if (i + 1) <= first_stage_steps:\n",
    "                    latent_model_input = torch.cat([latents] * 2)\n",
    "                    noise_pred = unet(latent_model_input, t, \n",
    "                                      encoder_hidden_states=torch.cat([uncond_embeddings, emb0])).sample                \n",
    "                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "                else:\n",
    "                    latent_model_input = torch.cat([latents] * 4)\n",
    "                    noise_pred = unet(latent_model_input, t, \n",
    "                                      encoder_hidden_states=torch.cat([uncond_embeddings, emb0, emb_add, emb_minus])).sample                \n",
    "                    noise_pred_uncond, noise_pred_text, noise_pred_text_plus, noise_pred_text_minus = noise_pred.chunk(4)\n",
    "                    noise_pred_text += (noise_pred_text_plus - noise_pred_text_minus) * correction_scale\n",
    "                    \n",
    "                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "                latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "            \n",
    "    with torch.no_grad():\n",
    "        latents = 1 / 0.18215 * latents\n",
    "        image = vae.decode(latents).sample\n",
    "\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)\n",
    "    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "    images = (image * 255).round().astype(\"uint8\")\n",
    "\n",
    "    return Image.fromarray(images[0])\n",
    "\n",
    "def transfer_prompt(emb0, emb_add, emb_minus, latents, \n",
    "                    num_inference_steps = 200, guidance_scale = 10, strength = 1):\n",
    "    emb = emb0 - emb_minus + emb_add\n",
    "    emb /= torch.sqrt((emb**2).sum())\n",
    "    emb *= torch.sqrt((emb0**2).sum())\n",
    "    \n",
    "    uncond_input = tokenizer(\n",
    "      [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
    "    )\n",
    "    uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n",
    "    \n",
    "    # latents = latents * scheduler.init_noise_sigma\n",
    "    latents = latents.to(torch_device) \n",
    "\n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "    \n",
    "    t0 = int(num_inference_steps * strength)\n",
    "    first_stage_steps = num_inference_steps - t0\n",
    "    \n",
    "    with autocast(\"cuda\"):\n",
    "        for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
    "            with torch.no_grad():\n",
    "                if (i + 1) <= first_stage_steps:\n",
    "                    emb_ = emb0\n",
    "                else:\n",
    "                    emb_ = emb\n",
    "                latent_model_input = torch.cat([latents] * 2)\n",
    "                noise_pred = unet(latent_model_input, t, \n",
    "                                  encoder_hidden_states=torch.cat([uncond_embeddings, emb_])).sample                \n",
    "                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "                        \n",
    "                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "                latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "            \n",
    "    with torch.no_grad():\n",
    "        latents = 1 / 0.18215 * latents\n",
    "        image = vae.decode(latents).sample\n",
    "\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)\n",
    "    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "    images = (image * 255).round().astype(\"uint8\")\n",
    "\n",
    "    return Image.fromarray(images[0])\n",
    "\n",
    "\n",
    "def concept_proj(emb0, emb_z0, emb_z1, emb_z_target,  latents0, \n",
    "                 strength = 1, num_inference_steps=200, guidance_scale=10): \n",
    "    uncond_input = tokenizer(\n",
    "      [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
    "    )\n",
    "    uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n",
    "    \n",
    "\n",
    "    latents = latents0.clone()\n",
    "    # latents = latents * scheduler.init_noise_sigma\n",
    "    latents = latents.to(torch_device) \n",
    "\n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "    \n",
    "    t0 = int(num_inference_steps * strength)\n",
    "    t0 = min(t0, num_inference_steps)\n",
    "    first_stage_steps = num_inference_steps - t0\n",
    "    \n",
    "\n",
    "    with autocast(\"cuda\"):\n",
    "        for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
    "            with torch.no_grad():\n",
    "                if (i + 1) <= first_stage_steps:\n",
    "                    latent_model_input = torch.cat([latents] * 2)\n",
    "                    noise_pred = unet(latent_model_input, t, \n",
    "                                      encoder_hidden_states=torch.cat([uncond_embeddings, emb0])).sample                \n",
    "                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "                else:\n",
    "                    latent_model_input = torch.cat([latents] * 5)\n",
    "                    noise_pred = unet(latent_model_input, t, \n",
    "                                      encoder_hidden_states=torch.cat([uncond_embeddings, emb0, emb_z0, emb_z1, emb_z_target])).sample                \n",
    "                    noise_pred_uncond, noise_pred_text0, noise_pred_text_z0, noise_pred_text_z1, noise_pred_text_z_target = noise_pred.chunk(5)\n",
    "                    \n",
    "                    ## score difference\n",
    "                    noise_tmp = noise_pred_text0 - noise_pred_text_z_target                    \n",
    "                    ## Z direction\n",
    "                    u = noise_pred_text_z1 - noise_pred_text_z0\n",
    "                    u /= torch.sqrt((u**2).sum())\n",
    "                    ## project out Z direction\n",
    "                    noise_pred_text0 -= (noise_tmp * u).sum() * u\n",
    "                    \n",
    "                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text0 - noise_pred_uncond)\n",
    "                latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "            \n",
    "    with torch.no_grad():\n",
    "        latents = 1 / 0.18215 * latents\n",
    "        image = vae.decode(latents).sample\n",
    "\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)\n",
    "    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "    images = (image * 255).round().astype(\"uint8\")\n",
    "    \n",
    "    return Image.fromarray(images[0])\n",
    "\n",
    "    \n",
    "\n",
    "def image_grid(imgs, rows, cols):\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid\n",
    "\n",
    "\n",
    "def experiment0(prompt, name, num_inference_steps, guidance_scale, \n",
    "               N, nrow, ncol, height = 512, width = 512, seeds = None):\n",
    "    ims = []\n",
    "    emb0 = get_text_embeddings(prompt) \n",
    "    if seeds is None:\n",
    "        seeds = [i for i in range(N)]\n",
    "    for i in seeds:\n",
    "        generator = torch.manual_seed(i)\n",
    "        lat = torch.randn(\n",
    "         (1, unet.in_channels, height // 8, width // 8),\n",
    "         generator=generator,\n",
    "        )\n",
    "        ims.append(im_gen(emb0, lat, num_inference_steps=num_inference_steps, guidance_scale=guidance_scale))\n",
    "\n",
    "    grid = image_grid(ims, rows=nrow, cols=ncol)\n",
    "    grid.save(f\"{name}.png\")\n",
    "        \n",
    "def experiment_proj(prompts, name, num_inference_steps, guidance_scale, \n",
    "               N, nrow, ncol, height = 512, width = 512, seeds = None):\n",
    "    ims_pj = []\n",
    "\n",
    "    emb0 = get_text_embeddings(prompts[0]) \n",
    "    emb_z0 = get_text_embeddings(prompts[1]) \n",
    "    emb_z1 = get_text_embeddings(prompts[2]) \n",
    "    emb_z_target = get_text_embeddings(prompts[3]) \n",
    "\n",
    "    if seeds is None:\n",
    "        seeds = [i for i in range(N)]\n",
    "    for i in seeds:\n",
    "        generator = torch.manual_seed(i)\n",
    "        lat = torch.randn(\n",
    "         (1, unet.in_channels, height // 8, width // 8),\n",
    "         generator=generator,\n",
    "        )\n",
    "        ims_pj.append(concept_proj(emb0, emb_z0, emb_z1, emb_z_target, lat, num_inference_steps=num_inference_steps, guidance_scale=guidance_scale))\n",
    "\n",
    "    grid = image_grid(ims_pj, rows=nrow, cols=ncol)\n",
    "    grid.save(f\"{name}.png\")\n",
    "    \n",
    "def experiment_transfer_score(prompts, name, num_inference_steps, guidance_scale, \n",
    "               N, nrow, ncol, s = 1, height = 512, width = 512, seeds = None):\n",
    "    ims = []\n",
    "    emb0 = get_text_embeddings(prompts[0]) \n",
    "    emb_plus = get_text_embeddings(prompts[1]) \n",
    "    emb_minus = get_text_embeddings(prompts[2]) \n",
    "\n",
    "    if seeds is None:\n",
    "        seeds = [i for i in range(N)]\n",
    "    for i in seeds:\n",
    "        generator = torch.manual_seed(i)\n",
    "        lat = torch.randn(\n",
    "         (1, unet.in_channels, height // 8, width // 8),\n",
    "         generator=generator,\n",
    "        )\n",
    "        ims.append(transfer_score(emb0, emb_plus, emb_minus, lat, num_inference_steps=num_inference_steps, guidance_scale=guidance_scale, strength=s))\n",
    "    grid = image_grid(ims, rows=nrow, cols=ncol)\n",
    "    grid.save(f\"{name}.png\")\n",
    "    \n",
    "def experiment_transfer_prompt(prompts, name, num_inference_steps, guidance_scale, \n",
    "               N, nrow, ncol, s = 1, height = 512, width = 512, seeds = None):\n",
    "    ims = []\n",
    "    emb0 = get_text_embeddings(prompts[0]) \n",
    "    emb_plus_prompt = get_text_embeddings(prompts[1]) \n",
    "    emb_minus_prompt = get_text_embeddings(prompts[2]) \n",
    "\n",
    "    if seeds is None:\n",
    "        seeds = [i for i in range(N)]\n",
    "    for i in seeds:\n",
    "        generator = torch.manual_seed(i)\n",
    "        lat = torch.randn(\n",
    "         (1, unet.in_channels, height // 8, width // 8),\n",
    "         generator=generator,\n",
    "        )\n",
    "        ims.append(transfer_prompt(emb0, emb_plus_prompt, emb_minus_prompt, lat, num_inference_steps=num_inference_steps, guidance_scale=guidance_scale, strength=s))\n",
    "\n",
    "    grid = image_grid(ims, rows=nrow, cols=ncol)\n",
    "    grid.save(f\"{name}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6845e711-cc25-4322-9d24-4473e13499f5",
   "metadata": {},
   "source": [
    "# Concept Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a03469-0a2a-4c28-a3fb-edb5b551f267",
   "metadata": {},
   "source": [
    "## King to Queen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d351a35b-3be5-4285-9851-617eab4cc478",
   "metadata": {},
   "outputs": [],
   "source": [
    "## KING-QUEEN example\n",
    "guidance_scale = 15\n",
    "num_inference_steps = 50\n",
    "N, nrow, ncol = 5, 1, 5\n",
    "\n",
    "experiment0(\"the portrait of a king\", 'king', num_inference_steps, guidance_scale, N, nrow, ncol)\n",
    "\n",
    "prompts = [\"the portrait of a king\", \"the portrait of a woman\", \"the portrait of a man\"]\n",
    "experiment_transfer_score(prompts, \"king2queen_score\", num_inference_steps, guidance_scale, N, nrow, ncol)\n",
    "\n",
    "prompts = [\"the portrait of a king\", \"the portrait of a woman\", \"the portrait of a man\"]\n",
    "experiment_transfer_prompt(prompts, \"king2queen_prompt\", num_inference_steps, guidance_scale, N, nrow, ncol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4902b8-e7df-4655-8dff-802ee861a085",
   "metadata": {},
   "source": [
    "## Frog, cartoon to photorealistic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6945f836",
   "metadata": {},
   "source": [
    "First, we generate 50 images for different styles. From them we selected a few for deomonstrations in the paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa5bc7d-d642-4f11-8bad-366de6030a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## FROG, direct prompting, full\n",
    "guidance_scale = 15\n",
    "num_inference_steps = 50\n",
    "N, nrow, ncol = 50, 5, 10\n",
    "\n",
    "names = ['frog_nostyle', 'frog_cartoon', 'frog_photo']\n",
    "prompts = ['a frog playing the piano, anthropomorphic',\n",
    "          'a frog playing the piano, anthropomorphic, cartoon',\n",
    "          'a frog playing the piano, anthropomorphic, photorealistic']\n",
    "\n",
    "experiment0(prompts[0], names[0], num_inference_steps, guidance_scale, N, nrow, ncol)\n",
    "experiment0(prompts[1], names[1], num_inference_steps, guidance_scale, N, nrow, ncol)\n",
    "experiment0(prompts[2], names[2], num_inference_steps, guidance_scale, N, nrow, ncol)\n",
    "\n",
    "name = 'frog_TransferScore'\n",
    "prompts = ['a frog playing the piano, anthropomorphic, cartoon',\n",
    "          'photorealistic','cartoon']\n",
    "experiment_transfer_score(prompts, name, num_inference_steps, guidance_scale, N, nrow, ncol)\n",
    "\n",
    "name = 'frog_TransferPrompt_bad'\n",
    "experiment_transfer_prompt(prompts, name, num_inference_steps, guidance_scale, N, nrow, ncol)\n",
    "\n",
    "name = 'frog_TransferPrompt_good'\n",
    "prompts = ['a frog playing the piano, anthropomorphic, cartoon',\n",
    "           'a man playing the piano, anthropomorphic, photorealistic',\n",
    "           'a man playing the piano, anthropomorphic, cartoon']\n",
    "experiment_transfer_prompt(prompts, name, num_inference_steps, guidance_scale, N, nrow, ncol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d9d451-b671-42c1-81f8-ee4577946e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [5, 9, 15, 20, 23, 29, 30, 39, 44, 46]\n",
    "\n",
    "N, nrow, ncol = 10, 1, 10\n",
    "names = ['frog_nostyle_selected', 'frog_cartoon_selected', 'frog_photo_selected']\n",
    "experiment0(prompts[0], names[0], num_inference_steps, guidance_scale, N, nrow, ncol, seeds = seeds)\n",
    "experiment0(prompts[1], names[1], num_inference_steps, guidance_scale, N, nrow, ncol, seeds = seeds)\n",
    "experiment0(prompts[2], names[2], num_inference_steps, guidance_scale, N, nrow, ncol, seeds = seeds)\n",
    "\n",
    "name = 'frog_selected_TransferScore'\n",
    "prompts = ['a frog playing the piano, anthropomorphic, cartoon',\n",
    "          'photorealistic','cartoon']\n",
    "experiment_transfer_score(prompts, name, num_inference_steps, guidance_scale, N, nrow, ncol, seeds = seeds)\n",
    "\n",
    "name = 'frog_selected_TransferPrompt_bad'\n",
    "experiment_transfer_prompt(prompts, name, num_inference_steps, guidance_scale, N, nrow, ncol, seeds = seeds)\n",
    "\n",
    "name = 'frog_selected_TransferPrompt_good'\n",
    "prompts = ['a frog playing the piano, anthropomorphic, cartoon',\n",
    "           'a man playing the piano, anthropomorphic, photorealistic', \n",
    "           'a man playing the piano, anthropomorphic, cartoon']\n",
    "experiment_transfer_prompt(prompts, name, num_inference_steps, guidance_scale, N, nrow, ncol, seeds = seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24da2d1b-2166-4a77-98e6-be6246fb4075",
   "metadata": {},
   "outputs": [],
   "source": [
    "guidance_scale = 15\n",
    "num_inference_steps = 50\n",
    "seeds = [15, 20, 39, 42, 43]\n",
    "N, nrow, ncol = 5, 1, 5\n",
    "\n",
    "prompts = ['a frog playing the piano, anthropomorphic',\n",
    "          'a frog playing the piano, anthropomorphic, cartoon',\n",
    "          'a frog playing the piano, anthropomorphic, photorealistic']\n",
    "\n",
    "names = ['frog_nostyle_5', 'frog_cartoon_5', 'frog_photo_5']\n",
    "experiment0(prompts[0], names[0], num_inference_steps, guidance_scale, N, nrow, ncol, seeds = seeds)\n",
    "experiment0(prompts[1], names[1], num_inference_steps, guidance_scale, N, nrow, ncol, seeds = seeds)\n",
    "experiment0(prompts[2], names[2], num_inference_steps, guidance_scale, N, nrow, ncol, seeds = seeds)\n",
    "\n",
    "name = 'frog_5_TransferScore'\n",
    "prompts = ['a frog playing the piano, anthropomorphic, cartoon',\n",
    "          'photorealistic','cartoon']\n",
    "experiment_transfer_score(prompts, name, num_inference_steps, guidance_scale, N, nrow, ncol, seeds = seeds)\n",
    "\n",
    "name = 'frog_5_TransferPrompt_bad'\n",
    "prompts = ['a frog playing the piano, anthropomorphic, cartoon',\n",
    "           'photorealistic', 'cartoon']\n",
    "experiment_transfer_prompt(prompts, name, num_inference_steps, guidance_scale, N, nrow, ncol, seeds = seeds)\n",
    "\n",
    "name = 'frog_5_TransferPrompt_good'\n",
    "prompts = ['a frog playing the piano, anthropomorphic, cartoon',\n",
    "           'a man playing the piano, anthropomorphic, photorealistic', \n",
    "           'a man playing the piano, anthropomorphic, cartoon']\n",
    "experiment_transfer_prompt(prompts, name, num_inference_steps, guidance_scale, N, nrow, ncol, seeds = seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca923e5-56f8-4982-8884-716df58148a4",
   "metadata": {},
   "source": [
    "## Failure example: nurse, deer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008b5818-3ec8-4152-a1f1-1bb06bb11631",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NURSE-DEER example\n",
    "guidance_scale = 10\n",
    "num_inference_steps = 50\n",
    "N, nrow, ncol = 10, 2, 5\n",
    "\n",
    "experiment0(\"a nurse sitting in a white room\", 'nurse_direct_short', num_inference_steps, guidance_scale, N, nrow, ncol)\n",
    "\n",
    "prompts = [\"a nurse sitting in a white room\", \"a buck on the grass\", \"a doe on the grass\"]\n",
    "experiment_transfer_score(prompts, \"nurse_deer_failure\", num_inference_steps, guidance_scale, N, nrow, ncol)\n",
    "\n",
    "## analyze reasons of failure\n",
    "names = ['buck', 'doe']\n",
    "prompts = [\"a buck on the grass\", \"a doe on the grass\"]\n",
    "experiment0(prompts[0], names[0], num_inference_steps, guidance_scale, N, nrow, ncol)\n",
    "experiment0(prompts[1], names[1], num_inference_steps, guidance_scale, N, nrow, ncol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f46b51-c0a8-4a5b-87e2-8fe96b2ee4cd",
   "metadata": {},
   "source": [
    "## Failure example: dog, renaissance, man"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e1a286-63aa-4453-b62d-661381f5e930",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dog-Renaissance example\n",
    "guidance_scale = 10\n",
    "num_inference_steps = 50\n",
    "N, nrow, ncol = 10, 2, 5\n",
    "\n",
    "experiment0(\"a dog sitting on the beach, cartoon\", 'dog_cartoon', num_inference_steps, guidance_scale, N, nrow, ncol)\n",
    "\n",
    "prompts = [\"a dog sitting on the beach, cartoon\", \"a man, renaissance-style painting\", \"a man, cartoon\"]\n",
    "experiment_transfer_score(prompts, \"dog_cartoon2renaissance_failure\", num_inference_steps, guidance_scale, N, nrow, ncol)\n",
    "\n",
    "## analyze reasons of failure\n",
    "names = ['man_cartoon', 'man_renaissance']\n",
    "prompts = [\"a man, cartoon\", \"a man, renaissance-style painting\"]\n",
    "experiment0(prompts[0], names[0], num_inference_steps, guidance_scale, N, nrow, ncol)\n",
    "experiment0(prompts[1], names[1], num_inference_steps, guidance_scale, N, nrow, ncol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04414842-a6ad-4f53-ad33-ff796e1c4796",
   "metadata": {},
   "source": [
    "# Concept Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4abf7f-0ffb-43e9-b5ca-e9d876ed9dc6",
   "metadata": {},
   "source": [
    "## labrador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076e1223-73a7-46b5-9d0d-b71720f41cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "guidance_scale = 10\n",
    "N, nrow, ncol = 20, 2, 10\n",
    "num_inference_steps = 50\n",
    "experiment0(\"a labrador\", 'labrador_plain', num_inference_steps, guidance_scale, N, nrow, ncol)\n",
    "\n",
    "experiment0(\"a baby labrador on the grass\", 'labrador_direct', num_inference_steps, guidance_scale, N, nrow, ncol)\n",
    "prompts = [\"a baby labrador on the grass\", \n",
    "           \"a light-colored labrador\", \"a dark-colored labrador\", \n",
    "           \"a labrador\"]\n",
    "experiment_proj(prompts, \"labrador_proj\", num_inference_steps, guidance_scale, N, nrow, ncol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6172e83-089d-47f6-90ba-e42a97b118b5",
   "metadata": {},
   "source": [
    "## nurse\n",
    "\n",
    "For this example, we find setting more steps results in images of better qualities. Therefore we choose 500 steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905a813a-47d2-4637-be14-5fe1c4a5867e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## nurse\n",
    "guidance_scale = 10\n",
    "N, nrow, ncol = 20, 2, 10\n",
    "num_inference_steps = 500\n",
    "experiment0(\"a nurse sitting in a white room\", 'nurse_direct', num_inference_steps, guidance_scale, N, nrow, ncol)\n",
    "prompts = [\"a nurse sitting in a white room\", \"a woman\", \"a man\", \"a person\"]\n",
    "experiment_proj(prompts, \"nurse_proj\", num_inference_steps, guidance_scale, N, nrow, ncol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52276628-45e9-494e-9d3c-badbef74bc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "## nurse\n",
    "guidance_scale = 10\n",
    "N, nrow, ncol = 20, 2, 10\n",
    "num_inference_steps = 500\n",
    "\n",
    "name = 'nurse_TransferPrompt_bad'\n",
    "prompts = ['a nurse sitting in a white room',\n",
    "           'person', 'woman']\n",
    "experiment_transfer_prompt(prompts, name, num_inference_steps, guidance_scale, N, nrow, ncol)\n",
    "\n",
    "name = 'nurse_TransferPrompt_good'\n",
    "prompts = ['a nurse sitting in a white room',\n",
    "           'a person sitting in a white room', \n",
    "           'a woman sitting in a white room']\n",
    "experiment_transfer_prompt(prompts, name, num_inference_steps, guidance_scale, N, nrow, ncol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d7efdf-46c6-4498-8de1-758495119c79",
   "metadata": {},
   "source": [
    "# mathematician"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9abd78c-2c30-4344-9ff3-64ca4199fd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## mathematician\n",
    "guidance_scale = 10\n",
    "N, nrow, ncol = 20, 2, 10\n",
    "num_inference_steps = 50\n",
    "experiment0(\"a person\", 'person_plain', num_inference_steps, guidance_scale, N, nrow, ncol)\n",
    "experiment0(\"a portrait of a mathematician\", 'mathematician_direct', num_inference_steps, guidance_scale, N, nrow, ncol)\n",
    "\n",
    "prompts = [\"a portrait of a mathematician\", \"a woman\", \"a man\", \"a person\"]\n",
    "experiment_proj(prompts, \"mathematician_proj\", num_inference_steps, guidance_scale, N, nrow, ncol)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-12.m99",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-12:m99"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
